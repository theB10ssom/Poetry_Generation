{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWVCFWZ7Oi6t/Z+/QRU9JM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "AeQQMqscX-sb",
        "outputId": "2c0f265a-36d2-4e41-a9ec-ed8cfcd5d625"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a22ce06-aa47-4e5b-881f-07801c13e303\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a22ce06-aa47-4e5b-881f-07801c13e303\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving troll_poem.csv to troll_poem.csv\n",
            "User uploaded file \"troll_poem.csv\" with length 19179 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSuhWio9Ys4B",
        "outputId": "90af691b-254d-4e40-ac19-463dbf073c8f"
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 36.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 26.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqpjlGI8YpST"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2TokenizerFast, BertTokenizerFast, GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7NCag01YlWY"
      },
      "source": [
        "root = pd.read_csv('/content/troll_poem.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlKQR6QHYq-u"
      },
      "source": [
        "root.content = root.content.str.replace('.', ' . ')\n",
        "root.content = root.content.str.replace(' <br>', '\\1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvPgJmZQjVwx",
        "outputId": "c185bb0c-4406-467d-ada0-aa1710a3d1fb"
      },
      "source": [
        "root.content.apply(lambda x: x.split()).apply(len).describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     37.000000\n",
              "mean      53.891892\n",
              "std       25.799207\n",
              "min       13.000000\n",
              "25%       33.000000\n",
              "50%       52.000000\n",
              "75%       69.000000\n",
              "max      101.000000\n",
              "Name: content, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNIMa7S4ZRRU"
      },
      "source": [
        "RANDOM_SEED = 821\n",
        "BATCH_SIZE = 2\n",
        "EPOCHS = 10\n",
        "MAX_LEN = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-Wkw2b7ZbP_",
        "outputId": "5e25f867-3dc0-4654-de59-d434b3403859"
      },
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"kykim/gpt3-kor-small_based_on_gpt2\")\n",
        "\n",
        "special_tokens_dict = {\n",
        "    'bos_token': '<BOS>', \n",
        "    'eos_token': '<EOS>', \n",
        "    'pad_token': '<PAD>'}\n",
        "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "tokenizer.add_tokens('\\1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'BertTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMAf8GFZbikZ"
      },
      "source": [
        "class PoemDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, gpt2_type='gpt2', max_length=MAX_LEN):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        \n",
        "        for i in data:\n",
        "            encodings_dict = tokenizer('<BOS>' + i + '<EOS>',\n",
        "                                     truncation=True,\n",
        "                                     max_length=max_length,\n",
        "                                     padding='max_length')\n",
        "\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids'][1:]))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask'][1:]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n",
        "        \n",
        "poem_dataset = PoemDataset(root['content'].values, tokenizer, max_length=MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGYgwtYqbyYi"
      },
      "source": [
        "def train_val_split(split, dataset):\n",
        "    train_size = int(split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    return train_size, val_size\n",
        "poem_line_train_size, poem_line_val_size = train_val_split(0.9, poem_dataset)\n",
        "poem_line_train_dataset, poem_line_val_dataset = random_split(poem_dataset, [poem_line_train_size, poem_line_val_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRJgA4m_cJss",
        "outputId": "aeac3fa9-563f-48d6-f1b0-fb4e56580a75"
      },
      "source": [
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f395c3af890>"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoM4ZRbzcQaM"
      },
      "source": [
        "poem_train_dataloader = DataLoader(poem_line_train_dataset,\n",
        "                              sampler=RandomSampler(poem_line_train_dataset),\n",
        "                              batch_size=BATCH_SIZE)\n",
        "\n",
        "poem_val_dataloader = DataLoader(poem_line_val_dataset,\n",
        "                            sampler=SequentialSampler(poem_line_val_dataset),\n",
        "                            batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqO484PXcXkc"
      },
      "source": [
        "# helper function for logging time\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 5e-4\n",
        "eps = 1e-8\n",
        "warmup_steps = 100\n",
        "\n",
        "# create text generation seed prompt\n",
        "device = torch.device('cuda')\n",
        "\n",
        "prompt = \"오늘\"\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYzU7gHQccep",
        "outputId": "441b8c70-375b-418c-9396-23237ed27225"
      },
      "source": [
        "configuration = GPT2Config(vocab_size=len(tokenizer), n_positions=MAX_LEN).from_pretrained(\"kykim/gpt3-kor-small_based_on_gpt2\", output_hidden_states=True)\n",
        "\n",
        "poem_model = GPT2LMHeadModel.from_pretrained(\"kykim/gpt3-kor-small_based_on_gpt2\",\n",
        "                                             config=configuration)\n",
        "\n",
        "poem_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "poem_model.cuda()\n",
        "optimizer = AdamW(poem_model.parameters(), lr=learning_rate, eps=eps)\n",
        "\n",
        "total_steps = len(poem_train_dataloader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=warmup_steps,\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "start_time = time.time()\n",
        "poem_model = poem_model.to(device)\n",
        "\n",
        "for epoch_i in range(0, EPOCHS):\n",
        "\n",
        "    print(f'Epoch {epoch_i + 1} of {EPOCHS}')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    poem_model.train()\n",
        "\n",
        "    for step, batch in enumerate(poem_train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        poem_model.zero_grad()        \n",
        "\n",
        "        outputs = poem_model(b_input_ids,\n",
        "                             labels=b_labels,\n",
        "                             attention_mask=b_masks,\n",
        "                             token_type_ids=None)\n",
        "\n",
        "        loss = outputs[0]  \n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(poem_train_dataloader)       \n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(f'Average Training Loss: {avg_train_loss}. Epoch Training Time: {training_time}')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    poem_model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in poem_val_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            outputs  = poem_model(b_input_ids,\n",
        "                                  attention_mask=b_masks,\n",
        "                                  labels=b_labels)\n",
        "\n",
        "            loss = outputs[0]  \n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss        \n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(poem_val_dataloader)\n",
        "\n",
        "\n",
        "    print(f'Average Validation Loss: {avg_val_loss}')\n",
        "\n",
        "print(f'Total Training Time: {format_time(time.time()-start_time)}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 10\n",
            "Average Training Loss: 8.575891606947955. Epoch Training Time: 0:00:07\n",
            "Average Validation Loss: 7.25028920173645\n",
            "Epoch 2 of 10\n",
            "Average Training Loss: 7.680476104511934. Epoch Training Time: 0:00:07\n",
            "Average Validation Loss: 6.3503577709198\n",
            "Epoch 3 of 10\n",
            "Average Training Loss: 5.674006546244902. Epoch Training Time: 0:00:07\n",
            "Average Validation Loss: 4.1422858238220215\n",
            "Epoch 4 of 10\n",
            "Average Training Loss: 2.988982488127316. Epoch Training Time: 0:00:06\n",
            "Average Validation Loss: 2.4552539587020874\n",
            "Epoch 5 of 10\n",
            "Average Training Loss: 1.1463229375727035. Epoch Training Time: 0:00:07\n",
            "Average Validation Loss: 2.0547581911087036\n",
            "Epoch 6 of 10\n",
            "Average Training Loss: 0.6561871889759513. Epoch Training Time: 0:00:06\n",
            "Average Validation Loss: 2.1788527965545654\n",
            "Epoch 7 of 10\n",
            "Average Training Loss: 0.43205952644348145. Epoch Training Time: 0:00:06\n",
            "Average Validation Loss: 2.269427180290222\n",
            "Epoch 8 of 10\n",
            "Average Training Loss: 0.28767315342145805. Epoch Training Time: 0:00:06\n",
            "Average Validation Loss: 2.348660171031952\n",
            "Epoch 9 of 10\n",
            "Average Training Loss: 0.20226450089146109. Epoch Training Time: 0:00:06\n",
            "Average Validation Loss: 2.3847711086273193\n",
            "Epoch 10 of 10\n",
            "Average Training Loss: 0.16765688710352955. Epoch Training Time: 0:00:06\n",
            "Average Validation Loss: 2.387143135070801\n",
            "Total Training Time: 0:01:08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpGKSeMQo_c6"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Boew2PfVdZvh"
      },
      "source": [
        "poem_model.eval()\n",
        "\n",
        "def poet_generator(poet, MAX_LEN=70):\n",
        "    prompt = f\"<BOS> {poet}\"\n",
        "    generated = torch.tensor(tokenizer.encode(prompt)[1:]).unsqueeze(0)\n",
        "    generated = generated.to(device)\n",
        "\n",
        "    sample_outputs = poem_model.generate(\n",
        "                                    generated, \n",
        "                                    do_sample=True,   \n",
        "                                    top_k=30, \n",
        "                                    max_length=MAX_LEN,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=3,\n",
        "                                    repetition_penalty=1.5\n",
        "                                    )\n",
        "\n",
        "    for i, sample_output in enumerate(sample_outputs):\n",
        "        result = \"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True))\n",
        "        result = re.sub(r\"<br>\", \"\\n\", result)\n",
        "        print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIn_4MIPhsJr",
        "outputId": "a0ebdb2c-c91e-4119-8d08-2fb4d7c403e7"
      },
      "source": [
        "poet_generator(\"한 편의 시는\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 한 편의 시는 어떤 위로가 되고 용기가 되는 걸까. 시적 화두는 여전히 남아 있다. 고통은 삶의 최종 장이다. 고통을 이기기 위해서는 아픔도 이겨야 한다. 죽음으로 갚지 않아도 된다. 이 시를 읽을 때 쯤이면 나도 저 글에 닿았을 텐데 하는 생각이 스쳐 지나갔다. 어느새 내 손가락의 움직임이 두\n",
            "\n",
            "\n",
            "1: 한 편의 시는 내 시를 애워싸고 너와 누우면 너는 사라진다. 영원의 이름 석 자가 남은 동안 너를 맞이하겠지. 겨울잠을 자던 너의 등뒤에서 빛이 날 때면면 조각들이 춤추는 것을 보면 천 번을 접어야지만 할 달음박질처럼 접어지네요\n",
            "\n",
            "\n",
            "2: 한 편의 시는 삶을 살아가는 동안 필요한 것을 내어주고 얻지 못한 것들을 건네는 말들이겠지. 시와 함께 나누었던 행복은 이내 사라져버리고 맙니다. 남은 것들은 그저 빈자리의 시인이 될 뿐이고 영원히 볼 수 없는 것들일뿐이에요. 박노해에게서 배운 것도 별로 없이 그냥 지나버린 시간들 속에서 저는\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL4rM3jcId9Y"
      },
      "source": [
        "## 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyIpm2RUc7Oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9147f559-d9ac-4574-8593-4003458ccf4f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-nRkNSHmSow"
      },
      "source": [
        "home_directory = '/gdrive/My Drive/'\n",
        "torch.save(poem_model.state_dict(), home_directory + 'poem_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Se98g8mIfvd"
      },
      "source": [
        "## 모델 불러오기 및 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPbjzjmOIjRp",
        "outputId": "88fc34b0-4fa1-4448-c7c9-52db7c819c29"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ot0QmzCmf-p"
      },
      "source": [
        "import torch\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xbONRZWmnBD"
      },
      "source": [
        "model = torch.load(home_directory + 'poem_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCHyGgyemqza"
      },
      "source": [
        "def poet_generator(poet, MAX_LEN=70):\n",
        "    prompt = f\"<BOS> {poet}\"\n",
        "    generated = torch.tensor(tokenizer.encode(prompt)[1:]).unsqueeze(0)\n",
        "    generated = generated.to(device)\n",
        "\n",
        "    sample_outputs = poem_model.generate(\n",
        "                                    generated, \n",
        "                                    do_sample=True,   \n",
        "                                    top_k=50, \n",
        "                                    max_length=MAX_LEN,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=3,\n",
        "                                    repetition_penalty=3.0\n",
        "                                    )\n",
        "\n",
        "    for i, sample_output in enumerate(sample_outputs):\n",
        "        result = \"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True))\n",
        "        result = re.sub(r\"<br>\", \"\\n\", result)\n",
        "        print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFeNBW-Vmzms",
        "outputId": "a39968f8-8088-4160-ca09-b2aeff57e333"
      },
      "source": [
        "poet_generator(\"안녕 나의 친구.\", 80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:3 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 안녕 나의 친구.! 빛은 상처에 닿아 빛이 되고 달이 떠오르면 또다른 별빛이 된다 우리는 이 경험을 통해서 우리들의 삶을 긍정으로 가득 채우려 애쓴다 그러나 사실은 아무것도 할 수 없는 상태에 직면해버리고 맙니다 우리가 잘 하고 있는지조차 깨닫지 못하고 그 순간에는 그저 방탕한 생활뿐이죠 하지만 우리의 역사를 돌아보면 그들의 업적은 눈부시게 빛나는데요 어떤\n",
            "\n",
            "\n",
            "1: 안녕 나의 친구.뿐이고 터닝 포인트는 보이지 않아요 우리들의 고향은 어디나 마찬가지예요 그리고 그 곳은 영원히 잊혀지게 마련이지요 그러나 이 글의 주인공처럼 다시 피어오를 꽃을 위해 우리는 노력해야 해요 그래야 영원하겠지요 우리가 살고 있는 동네도 이제는 그들의 손짓에 의해 점령당하고 있어요 우리 모두 잘 살아보아요 저쪽에서 우리를 향해 다가오는 사람이 있겠지 하지만 언제까지 우리의 땅\n",
            "\n",
            "\n",
            "2: 안녕 나의 친구.하고 싶은 말 그대와 내가 나눌 말의 무게를 이기며 달려가는 말은 아무것도 할 수 없는 사람뿐이겠지, 모든 걸 놓아버리고 오직 내 발만 동동 굴리는 것만 남은 사람은 영원히 알 수가 없지 않을까 하는 생각에 주저하는 제가 볼 땐 슬픈 하루일 수도 희망적인 내일의 일수도 있겠군요 허허 그런 까닭에 저는 항상 지쳐있고 허무맹랑한\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0NEbnEtKwbF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}